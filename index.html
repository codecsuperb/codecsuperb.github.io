<!DOCTYPE html>
<!--
	Dopetrope by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Codec SUPERB</title>

	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="homepage is-preload">
	<div id="page-wrapper">

		<!-- Header -->
		<section id="header">
			<!-- Logo -->
			<h1><a href="index.html">Codec SUPERB</a></h1>

			<!-- Banner -->
			<section id="banner">
				<header>
					<h2>Codec SUPERB Challenge @ SLT 2024</h2>
					<p style="font-weight: 700">Codec Speech processing Universal PERformance Benchmark Challenge</p>
					<br>
					<ul class="actions">
						<li><a href="#timeline" class="button">Timeline</a></li>
						<li><a href="#committee" class="button">Committee</a></li>
						<li><a href="#contact" class="button">Contact</a></li>
						<li><a href="#keynote" class="button">Keynote</a></li>
						<li><a href="https://arxiv.org/abs/2409.14085" class="button">Paper</a></li>
					</ul>
				</header>
			</section>

			<section>
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<img style="margin-bottom: 20px" width=100% src="assets/figures/codec_papers.png" alt="" />
						<header>
							<h2>Introduction</h2>
						</header>
						<section style="line-height: 1.75em; font-size: 22px">
							<p>
								Neural audio codecs are initially introduced to compress audio data into compact codes
								to reduce transmission latency. Researchers recently discovered the potential of codecs
								as suitable tokenizers for converting continuous audio into discrete codes, which can be
								employed to develop audio language models (LMs). The neural audio codec's dual roles in
								minimizing data transmission latency and serving as tokenizers underscore its critical
								importance. The ideal neural audio codec models should preserve content,
								paralinguistics, speakers,
								and audio information. However, the
								question of which codec achieves optimal audio information preservation remains
								unanswered, as in different papers, models are evaluated on their selected experimental
								settings. There's a lack of a challenge to enable a fair comparison of all current
								existing codec models and stimulate the development of more advanced codecs. To fill
								this blank, we propose the Codec-SUPERB challenge.
								<br><br>
								The goal of this challenge is to encourage innovative methods and a comprehensive
								understanding of the capability of codec models. This challenge will conduct a
								comprehensive analysis to provide insights into codec models from both application and
								signal perspectives, diverging from previous codec papers that predominantly focus on
								signal-level comparisons following the paper <a href="https://arxiv.org/abs/2402.13071">
									Codec-SUPERB: An In-Depth
									Analysis of Sound Codec Models (Wu et al., arXiv 2024)</a>. The diverse set of
								signal-level metrics,
								including Perceptual Evaluation of Speech Quality (PESQ), Short-Time Objective
								Intelligibility (STOI), Mel distance, and Signal-to-Distortion Ratio (SDR) enable us to
								conduct a thorough evaluation of
								sound
								quality across various dimensions, encompassing spectral fidelity, temporal
								dynamics,
								perceptual clarity, and intelligibility. The application angle evaluation will
								comprehensively analyze each codec's ability to preserve crucial audio information,
								encompassing content, speaker timbre, emotion, and general audio characteristics. We
								hope this challenge can inspire innovative research in neural codec development.
								With
								this proposal, we aim to promote innovation in neural audio codec fields and
								advancing
								the research frontier.
							</p>
						</section>
					</article>
				</div>
			</section>
			<section id="keynote">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>keynote Speech</h2>
						</header>
						<section style="line-height: 1.75em; font-size: 22px">
							<strong>Date:</strong> Tuesday, December 3<br>
							<strong>Time:</strong> 15:00 - 18:30 (3.5 hours total)<br>
							<strong>Format:</strong> Each speaker will have a 30-minute talk followed by a 5-minute
							Q&A session.<br>
							<br>
							<strong>Notes</strong>

							<li>Parts of the recorded video will be posted on the event website after the
								session.</li>
							<li>Presentation slides will also be made available on the event website.</li>
							<br>
							<strong>Timeline</strong>
							<li> 15:10 - 15:45 Neil Zeghidour</li>
							<li> 15:45 - 16:20 Dongchao Yang</li>
							<li> 16:20 - 16:55 Shang-Wen Li</li>
							<li> 16:55 - 17:25 Paper Presentation</li>
							<li> 17:25 - 18:00 Wenwu Wang</li>
							<li> 18:00 - 18:35 Minje Kim</li>
						</section>

			</section>
			<section id="keynote-details">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>Keynote Speakers</h2>
						</header>

						<section style="line-height: 1.75em; font-size: 22px">
							<!-- Keynote Speaker Template -->

							<!-- Wenwu Wang -->
							<div class="keynote-speaker" style="margin-bottom: 30px;">
								<!-- Speaker Details -->
								<div style="text-align: center; margin-bottom: 20px;">
									<h3>
										<p><b>Speaker: Prof. Wenwu Wang</b></p>
									</h3>
									<!-- Speaker Photo -->
									<img src="assets/figures/wenwu.png" alt="Prof. Wenwu Wang"
										style="max-width: 150px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);" />
								</div>
								<!-- Speaker Content -->
								<div>
									<h3>Title: <em>Neural Audio Codecs: Recent Progress and a Case Study with
											SemantiCodec</em></h3>
									<section>
										<p><b>Abstract:</b>
											The neural audio codec has attracted increasing interest as a highly
											effective method for audio compression and representation. By transforming
											continuous audio into discrete tokens, it facilitates the use of large
											language modelling (LLM) techniques in audio processing. In this talk, we
											will report recent progress in neural audio codecs, with a particular focus
											on SemantiCodec, a new neural audio codec for ultra-low bit rate audio
											compression and tokenization. SemantiCodec features a dual-encoder
											architecture: a semantic encoder using a self-supervised AudioMAE,
											discretized using k-means clustering on extensive audio data, and an
											acoustic encoder to capture the remaining details. The semantic and acoustic
											encoder outputs are then used to reconstruct audio via a
											diffusion-model-based decoder. SemantiCodec presents several advantages over
											previous codecs, which typically operate at high bitrates, are confined to
											narrow domains like speech, and lack the semantic information essential for
											effective language modelling. First, SemantiCodec compresses audio into
											fewer than 100 tokens per second across various audio types, including
											speech, general audio, and music, while maintaining high-quality output.
											Second, it preserves substantially richer semantic information from audio
											compared to all evaluated codecs. We will illustrate these benefits through
											benchmarking and conclude by discussing potential directions for future
											research in this field.

										</p>
										<p><b>Bio:</b>
											Wenwu Wang is a Professor in Signal Processing and Machine Learning,
											University of Surrey, UK. He is also an AI Fellow at the Surrey Institute
											for People Centred Artificial Intelligence. His current research interests
											include signal processing, machine learning and perception, artificial
											intelligence, machine audition (listening), and statistical anomaly
											detection. He has (co)-authored over 300 papers in these areas. He has been
											recognized as a (co-)author or (co)-recipient of more than 15 accolades,
											including the 2022 IEEE Signal Processing Society Young Author Best Paper
											Award, ICAUS 2021 Best Paper Award, DCASE 2020 and 2023 Judge’s Award, DCASE
											2019 and 2020 Reproducible System Award, and LVA/ICA 2018 Best Student Paper
											Award. He is an Associate Editor (2020-2025) for IEEE/ACM Transactions on
											Audio Speech and Language Processing, and an Associate Editor (2024-2026)
											for IEEE Transactions on Multimedia. He was a Senior Area Editor (2019-2023)
											and Associate Editor (2014-2018) for IEEE Transactions on Signal Processing.
											He is the elected Chair (2023-2024) of IEEE Signal Processing Society (SPS)
											Machine Learning for Signal Processing Technical Committee, a Board Member
											(2023-2024) of IEEE SPS Technical Directions Board, the elected Chair
											(2025-2027) and Vice Chair (2022-2024) of the EURASIP Technical Area
											Committee on Acoustic Speech and Music Signal Processing, an elected Member
											(2021-2026) of the IEEE SPS Signal Processing Theory and Methods Technical
											Committee. He has been on the organising committee of INTERSPEECH 2022, IEEE
											ICASSP 2019 & 2024, IEEE MLSP 2013 & 2024, and SSP 2009. He is Technical
											Program Co-Chair of IEEE MLSP 2025. He has been an invited Keynote or
											Plenary Speaker on more than 20 international conferences and workshops.
										</p>
									</section>
								</div>
							</div>

							<!-- Minje -->
							<div class="keynote-speaker" style="margin-bottom: 30px;">
								<!-- Speaker Details -->
								<div style="text-align: center; margin-bottom: 20px;">
									<h3>
										<p><b>Speaker: Prof. Minje Kim</b></p>
									</h3>
									<!-- Speaker Photo -->
									<img src="assets/figures/minje.jpg" alt="Prof. Minje Kim"
										style="max-width: 150px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);" />
								</div>
								<!-- Speaker Content -->
								<div>
									<section>
										<p><b>Bio:</b>
											Minje Kim was an associate professor at Indiana University (2016-2023). From
											Jan. 2024 he moved to the School of Computing and Data Science at the
											University of Illinois at Urbana-Champaign. He is also a Visiting Academic
											working at Amazon Lab126. He earned his Ph.D. in Computer Science at UIUC
											(2016) after working as a researcher at ETRI, a national lab in Korea (2006
											to 2011). Prior to that, he received his Master’s and Bachelor’s degrees in
											the Department of Computer Science and Engineering at POSTECH (Summa Cum
											Laude) and in the Division of Information and Computer Engineering at Ajou
											University (with honors) in 2006 and 2004, respectively. During his career
											as a researcher, he has focused on developing machine learning models for
											audio signal processing applications. He is a recipient of various awards,
											including the NSF Career Award (2021), IU Trustees Teaching Award (2021),
											IEEE SPS Best Paper Award (2020), Google and Starkey’s grants for
											outstanding student papers in ICASSP 2013 and 2014, respectively, and
											Richard T. Cheng Endowed Fellowship from UIUC in 2011. He is an IEEE Senior
											Member and also belongs to the IEEE Audio and Acoustic Signal Processing
											Technical Committee as a member (2018-2023) and the Vice Chair (2024). He is
											serving as Senior Area Editor for IEEE/ACM Transactions on Audio, Speech,
											and Language Processing and IEEE Signal Processing Letters, Associate Editor
											for EURASIP Journal of Audio, Speech, and Music Processing, and Consulting
											Associate Editor for IEEE Open Journal of Signal Processing. He was the
											General Chair of IEEE WASPAA 2023 and also a reviewer, program committee
											member, or area chair for the major machine learning and signal processing
											venues. He is on more than 50 patents as an inventor.
										</p>
									</section>
								</div>
							</div>

							<!-- Dongchao Yang -->
							<div class="keynote-speaker" style="margin-bottom: 30px;">
								<!-- Speaker Details -->
								<div style="text-align: center; margin-bottom: 20px;">
									<h3>
										<p><b>Speaker: Dongchao Yang</b></p>
									</h3>
									<!-- Speaker Photo -->
									<img src="assets/figures/dongchao.jpg" alt="Dongchao Yang"
										style="max-width: 150px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);" />
								</div>
								<!-- Speaker Content -->
								<div>
									<h3>Title: <em>Challenges in Developing Universal Audio Foundation Model
										</em></h3>
									<section>
										<p><b>Abstract:</b>
											Building a universal audio foundation model for different audio
											generation tasks, such as text-to-speech, text-to-audio, singing voice
											synthesis, voice conversion, and speech dialogue, has attract great
											interest in the audio community. Audio codec and audio modeling strategies
											are two key points. In this talk, we review the development of audio
											codec and different audio modeling strategies in the literature, and
											show our recent works in audio codec and audio modeling methods. Lastly,
											we summarize the challenges and potential directions in development
											universal audio foundation model.
										</p>
										<p><b>Bio:</b>
											Dongchao Yang is a second-year Ph.D. student at The Chinese
											University of Hong Kong, supervised by Prof. Helen Meng. Before that, he
											received his master's degree from Peking University. His research
											interests encompass the extensive domain of speech and language
											intelligence, which includes audio foundation models, multi-modal large
											language models (MLLMs), text-to-speech synthesis (TTS), cross-modal
											representation learning, among other related areas. Currently, his work
											focuses on audio-text foundation models and audio codec models.

										</p>
									</section>
								</div>
							</div>

							<!-- Shang-Wen Li -->
							<div class="keynote-speaker" style="margin-bottom: 30px;">
								<!-- Speaker Details -->
								<div style="text-align: center; margin-bottom: 20px;">
									<h3>
										<p><b>Speaker: Dr. Shang-Wen Li</b></p>
									</h3>
									<!-- Speaker Photo -->
									<img src="assets/figures/shangwen.jpg" alt="Dr. Daniel Li"
										style="max-width: 150px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);" />
								</div>
								<!-- Speaker Content -->
								<div>
									<h3>Title: <em>VoiceCraft: Zero-Shot Speech Editing and TTS in the Wild</em></h3>
									<section>
										<p><b>Abstract:</b>
											We introduce VoiceCraft, a token infilling neural codec language model
											(NCLM),
											that achieves state-of-the-art (SOTA) performance on both speech editing and
											zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts.
											We
											study the case of developing VoiceCraft, to understand how NCLM works in
											solving
											critical speech generation problems, how codec sets the foundation of SOTA
											NCLM,
											and the challenges in the development and evaluation.
											<br>
											<br>
											VoiceCraft leverages Encodec to tokenize speech signals; it employs a
											Transformer decoder architecture and introduces a token rearrangement
											procedure
											that combines causal masking and delayed stacking to enable token generation
											within an existing sequence. On speech editing tasks, VoiceCraft produces
											edited
											speech that is nearly indistinguishable from unedited recordings in terms of
											naturalness, as evaluated by humans; for zero-shot TTS, our model
											outperforms
											prior SOTA models including VALLE and XTTS-v2. The models are evaluated on
											challenging and realistic datasets, that consist of diverse accents,
											speaking
											styles, recording conditions, and background noise and music, and our model
											performs consistently well compared to other models and real recordings. In
											particular, for speech editing evaluation, we introduce a high quality,
											challenging, and realistic dataset named RealEdit. We encourage audience to
											listen to the demos at <a
												href="https://jasonppy.github.io/VoiceCraft_web/">this
												website</a>.

										</p>
										<p><b>Bio:</b>
											Shang-Wen Li is a Research Lead and Manager at Meta’s Fundamental AI
											Research
											(FAIR) team, and he worked at Apple Siri, Amazon Alexa and AWS before
											joining
											FAIR. He completed his PhD in 2016 at MIT in the Spoken Language Systems
											group
											of Computer Science and Artificial Intelligence Laboratory (CSAIL). His
											recent
											research is focused on multimodal large language models, multimodal
											representation learning, and spoken language understanding.
										</p>
									</section>
								</div>
							</div>

							<!-- Neil Zeghidour -->
							<div class="keynote-speaker" style="margin-bottom: 30px;">
								<!-- Speaker Details -->
								<div style="text-align: center; margin-bottom: 20px;">
									<h3>
										<p><b>Speaker: Dr. Neil Zeghidour</b></p>
									</h3>
									<!-- Speaker Photo -->
									<img src="assets/figures/neil.jpg" alt="Dr. Neil Zeghidour"
										style="max-width: 150px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);" />
								</div>
								<!-- Speaker Content -->
								<div>
									<section>
										<p><b>Bio:</b>
											Founded Kyutai in Paris. Previously Staff Research Scientist at Google
											DeepMind. Teaching "Algorithms for Speech and Natural Language Processing"
											at master MVA (École Normale Supérieure). Interested in deep learning for
											audio understanding, audio synthesis, and signal processing.
										</p>
									</section>
								</div>
							</div>
							<!-- Add more speaker sections as needed -->
						</section>
					</article>
				</div>
			</section>

			<section id="timeline">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>News</h2>
						</header>
						<div class="ritz grid-container" dir="ltr">
							<section style="line-height: 1.75em; font-size: 22px">

								<li> <b>2024-11-22</b> Keynote Speakers information updated</li>
								<li> <b>2024-09-21</b> Codec-SUPERB @ SLT 2024 is available on <a
										href="https://arxiv.org/abs/2409.14085">arXiv</a></li>
								<li> <b>2024-06-10</b> Evaluation Colab Example [<a
										href="https://colab.research.google.com/drive/1tVZ_oe_eeRdsclAHTa72leUcIoycpMsa?usp=sharing">Colab
										Link</a>] </li>
								<li><b>2024-04-29</b>: Rule announced: [<a href="./Codec-SUPERB-rule.pdf">Rule with
										Baselines</a>]</li>
								Please use the <a href="https://forms.gle/sBRB4VsoDKkNYQQ98">Google Form</a> to
								register. <br>
								Please submit the evaluation results by creating a <a
									href="https://github.com/voidful/Codec-SUPERB/tree/SLT_Challenge">GitHub issue</a>
							</section>
			</section>
			<section id="timeline">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>Timeline / Important Dates</h2>
						</header>
						<!-- Create a timeline template -->
						<div class="ritz grid-container" dir="ltr">
							<section style="line-height: 1.75em; font-size: 22px">
								<li>Results announcement and hosting challenge: 2024-12</li>
								<li><del>Submission deadline: 2024-06-20</del></li>
								<li><del>Submission start: 2024-04-29</del></li>
								<li><del>Rule announcement: 2024-04-29</del> [<a href="./Codec-SUPERB-rule.pdf">Rule
										with Baselines</a>]</li>
								<li>Data available for public-set (Hidden-set will be hidden throughout the challenge)
								</li>

							</section>
			</section>
			<section id="committee">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>Organizers</h2>
						</header>
						<div class="ritz grid-container" dir="ltr">
							<!-- <h3>Organizers</h3> -->
							<section style="line-height: 1.75em; font-size: 22px">
								<h4> Academia </h4>
								<li>
									Hung-yi Lee (NTU) <a
										href="https://speech.ee.ntu.edu.tw/~hylee/index.php">website</a> </li>
								<li>Haibin Wu (NTU) <a href="https://hbwu-ntu.github.io/">website</a> </li>
								<li>Kai-Wei Chang (NTU) <a href="https://kwchang.org">website</a>
								</li>
								<li>Alexander H. Liu (MIT) <a href="https://alexander-h-liu.github.io/">website</a>
								</li>
								<li>Dongchao Yang (CUHK) <a href="https://dongchaoyang.top/">website</a> </li>
								<li>Shinji Watanabe (CMU) <a
										href="https://sites.google.com/view/shinjiwatanabe">website</a>
								</li>
								<li> James Glass (MIT) <a href="https://www.csail.mit.edu/person/jim-glass">website</a>
								</li>
								<h4> Industrial </h4>

								<li>Songxiang Liu (miHoYo) <a href="https://liusongxiang.github.io/">website</a> </li>

								<li>Yi-Chiao Wu (Meta) <a
										href="https://scholar.google.co.jp/citations?user=KKaOQVwAAAAJ">website</a>
								</li>
								<li>Xu Tan (Microsoft) <a
										href="https://www.microsoft.com/en-us/research/people/xuta/">website</a> </li>
							</section>
							<header>
								<h2>Technical Committee</h2>
							</header>
							<div class="ritz grid-container" dir="ltr">
								<section style="line-height: 1.75em; font-size: 22px">
									<li>
										Ho-Lam Chung (NTU)
									</li>
									<li>
										Yi-Cheng Lin (NTU)
									</li>
									<li>
										Yuan-Kuei Wu (NTU)
									</li>
									<li>
										Xuanjun Chen (NTU)
									</li>
									<li>
										Ke-Han Lu (NTU)
									</li>
									<li>
										Jiawei Du (NTU)
									</li>
								</section>
			</section>


			<section>
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>References</h2>
						</header>
						<section>
							<p>[1] Wu, Haibin, et al. "Towards audio language modeling-an overview." arXiv preprint
								arXiv:2402.13236 (2024).<br>
								[2] Wu, Haibin, et al. "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models." arXiv
								preprint arXiv:2402.13071 (2024).<br>
								[3] Neil Zeghidour et al., “Soundstream: An end-to-end neural audio codec,” IEEE/ACM
								Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495–507, 2021.<br>
								[4] Zalan Borsos et al., “Audiolm: a language modeling approach to audio generation,”
								IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.<br>
								[5] Felix Kreuk et al., “Audiogen: Textually guided audio generation,” arXiv preprint
								arXiv:2209.15352, 2022.<br>
								[6] Défossez, Alexandre, et al. "High fidelity neural audio compression." arXiv preprint
								arXiv:2210.13438 (2022).<br>
								[7] Chengyi Wang et al., “Neural codec language models are zero-shot text to speech
								synthesizers,” arXiv preprint arXiv:2301.02111, 2023.<br>
								[8] Andrea Agostinelli et al., “Musiclm: Generating music from text,” arXiv preprint
								arXiv:2301.11325, 2023.<br>
								[9] Ziqiang Zhang et al., “Speak foreign languages with your own voice: Cross-lingual
								neural
								codec language modeling,” arXiv preprint arXiv:2303.03926, 2023.<br>
								[10] Jenrungrot, Teerapat, et al. "LMCodec: A Low Bitrate Speech Codec with Causal
								Transformer Models." ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech
								and
								Signal Processing (ICASSP). IEEE, 2023.<br>
								[11] Tianrui Wang et al., “Viola: Unified codec language models for speech recognition,
								synthesis, and translation,” arXiv preprint arXiv:2305.16107, 2023.<br>
								[12] Jiang, Xue, et al. "Latent-Domain Predictive Neural Speech Coding." IEEE/ACM
								Transactions on Audio, Speech, and Language Processing (2023).<br>
								[13] Yi-Chiao Wu et al., “Audiodec: An open-source streaming high- fidelity neural audio
								codec,” in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and
								Signal
								Processing (ICASSP). IEEE, 2023, pp. 1–5.<br>
								[14] Dongchao Yang et al., “Hifi-codec: Group-residual vector quantization for high
								fidelity
								audio codec,” arXiv preprint arXiv:2305.02765, 2023.<br>
								[15] Borsos, Zalán, et al. "SoundStorm: Efficient Parallel Audio Generation." arXiv
								preprint
								arXiv:2305.09636 (2023).<br>
								[16] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar,
								“High-fidelity audio compression with improved rvqgan,” arXiv preprint arXiv:2306.06546,
								2023.<br>
								[17] Jade Copet et al., “Simple and controllable music generation,” arXiv preprint
								arXiv:2306.05284, 2023.<br>
								[18] Paul K Rubenstein et al., “Audiopalm: A large language model that can speak and
								listen,” arXiv preprint arXiv:2306.12925, 2023.<br>
								[19] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu, “Speechtokenizer:
								Unified speech tokenizer for speech large language models,” arXiv preprint
								arXiv:2308.16692,
								2023.<br>
								[20] Xiaofei Wang et al., “Speechx: Neural codec language model as a versatile speech
								transformer,” arXiv preprint arXiv:2308.06873, 2023.<br>
								[21] Ratnarajah, Anton, et al. "M3-AUDIODEC: Multi-channel multi-speaker multi-spatial
								audio
								codec." arXiv preprint arXiv:2309.07416 (2023).<br>
								[22] Xu, Zhongweiyang, et al. "SpatialCodec: Neural Spatial Speech Coding." arXiv
								preprint
								arXiv:2309.07432 (2023).<br>
								[23] Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng, “Funcodec: A fundamental,
								reproducible and integrable open-source toolkit for neural speech codec,” arXiv preprint
								arXiv:2309.07405, 2023.<br>
								[24] Qian Chen et al., “Lauragpt: Listen, attend, understand, and regenerate audio with
								gpt,” arXiv preprint arXiv:2310.04673, 2023.<br>
								[25] Dongchao Yang et al., “Uniaudio: An audio foundation model toward universal audio
								generation,” arXiv preprint arXiv:2310.00704, 2023.<br>
								[26] Ji, Shengpeng, et al. "Language-Codec: Reducing the Gaps Between Discrete Codec
								Representation and Speech Language Models." arXiv preprint arXiv:2402.12208 (2024).<br>
								[27] Liu, Haohe, et al. "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for
								General Sound." arXiv preprint arXiv:2405.00233 (2024).<br>
								[28] Ai, Yang, et al. "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase
								Spectrum Encoding and Decoding." arXiv preprint arXiv:2402.10533 (2024).</p>
						</section>


						<div class="ritz grid-container" dir="ltr">

			</section>
			<section id="paper">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>Codec-SUPERB @ SLT 2024 Paper</h2>
						</header>
						<section style="line-height: 1.75em; font-size: 22px">
							<b>Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models</b>
							<br>
							<!-- Thumbnail and Link -->
							<a href="https://arxiv.org/abs/2409.14085" target="_blank">
								<img src="assets/figures/paper.jpg" alt="Codec-SUPERB Paper Thumbnail"
									style="width: 400px; height: auto; margin-top: 15px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.2); border-radius: 5px;">
							</a>
							<br>
							[<a href="https://arxiv.org/abs/2409.14085">arXiv link</a>]
						</section>
					</article>
				</div>
			</section>


			<section id="contact">
				<div class="container">
					<!-- Content -->
					<article class="box post">
						<header>
							<h2>Contact</h2>
						</header>
						<section style="line-height: 1.75em; font-size: 22px">
							codecsuperb@gmail.com
						</section>
			</section>
	</div>
	</article>
	</div>

	</section>

	<!-- Footer -->
	<section id="footer">
		<div class="container"></div>
	</section>
	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.dropotron.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
</body>

</html>